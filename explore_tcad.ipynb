{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d55b5d10-4ba0-4c03-92bc-ee83666d8cd4",
   "metadata": {},
   "source": [
    "# TCAD file exploration\n",
    "\n",
    "We have received files from a client.  They are ....\n",
    "\n",
    "# Shorten files for browsing\n",
    "\n",
    "To shorten the files for browsing we can run a short shell script. This opens the zip that was received, and truncates each file at 100 lines long.\n",
    "\n",
    "```{bash, eval=F}\n",
    "# rm -rf shortened_appraisal_files\n",
    "unzip original_data/Appraisal_Roll_History_1990.zip -d shortened_appraisal_files\n",
    "find shortened_appraisal_files -name \"*.TXT\" -exec sed -i.full 100q {} \\;\n",
    "find shortened_appraisal_files -name \"*.TXT.full\" -exec rm {} \\;\n",
    "zip -r shortened_appraisal_files.zip shortened_appraisal_files\n",
    "```\n",
    "\n",
    "We can now attempt to load a shortened file using pandas. This shows that we know how the file is formatted. In this case it is a csv-like file, meaning that each row is a record and each column is separated by a pipe character: |."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14f20d2-0c9b-4ef3-8125-651a99c01e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df = pd.read_csv(\"shortened_appraisal_files/Appraisal_Roll_History_1990_A/TCBC_SUM_1990_JURIS.TXT\", sep = \"|\")\n",
    "df = pd.read_csv(\"~/TCBC_SUM_1990_JURIS\", sep = \"|\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ea1fd-ce05-441e-91ef-1ea869671783",
   "metadata": {},
   "source": [
    "Challenge now is to use the *.TDF files to create tables.  I can think of two approaches.\n",
    "\n",
    "1. The *.TDF files are all text files, and they contain SQL which defines tables, so if those are fed to duckdb they should be able to create tables into which the TXT pipe-separated CSV files can be read.  There may be issues with the datatypes not matching (which would require mapping the current datatype definitions to duckdb datatypes by changing the words used to give the datatype to the columns).\n",
    "\n",
    "2. Take the column names out of the TDF files and add them as the column names while reading the relevant CSV files into duckdb.  This would use duckdb's auto understanding of the column datatypes (so it would run, but it might guess wrongly and truncate or change data).\n",
    "\n",
    "I think we should explore step 1 first.\n",
    "\n",
    "## Creating tables using the TDF files\n",
    "\n",
    "We have TDF files scattered through the \\_A and \\_B folders.  I have created a schema (a namespace) for the files from \\_A called \"folder_A\" and \"folder_B\". So there are tables named the same thing in each of the schemas.  You can reference the tables as folder_A.TCBC_SUM_1990_JURIS and folder_B.TCBC_SUM_1990_JURIS \n",
    "\n",
    "We can use python to read each TDF file separately, create the table and then try to load the matching TXT file.  A little guidance on how to process a directory structure of files using Path and glob here:\n",
    "http://howisonlab.github.io/datawrangling/faq.html#get-data-from-filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "042f9a5a-bf21-410f-99ae-730c302e91a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf duckdb-file.db*\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect('duckdb-file.db') #  string to persist to disk\n",
    "# Python DBI API - abstract away differences between underlying database systems.\n",
    "cursor = con.cursor()\n",
    "\n",
    "file_directory = 'shortened_appraisal_files/'\n",
    "# limit_to_file = 'TCBC_SUM_1990_JURIS'\n",
    "limit_to_file = '*' # all files\n",
    "\n",
    "cursor.execute(\"CREATE SCHEMA IF NOT EXISTS folder_A;\")\n",
    "cursor.execute(\"CREATE SCHEMA IF NOT EXISTS folder_B;\")\n",
    "\n",
    "for filename in Path(file_directory).rglob(limit_to_file + '.TDF'):  # *.TDF\n",
    "    #print(filename.parts)\n",
    "    if \"_A\" in filename.parts[1]:\n",
    "        schema = \"folder_A\"\n",
    "    else:\n",
    "        schema = \"folder_B\"\n",
    "    \n",
    "    table_name = schema + \".\" + Path(filename).stem # e.g., A_TCBC_SUM_1990_JURIS\n",
    "\n",
    "    # read .TDF file into string\n",
    "    create_table_sql = Path(filename).read_text()\n",
    "    \n",
    "    # Need to alter table name to read in both _A and _B files\n",
    "    create_table_sql = create_table_sql.replace(Path(filename).stem, table_name)\n",
    "   \n",
    "    # Here we have the table creation code in a string, so we can\n",
    "    # swap datatypes out.\n",
    "    # tried SMALLDATETIME --> DATETIME but was still giving errors\n",
    "    # will need to fix this later.\n",
    "    create_table_sql = create_table_sql.replace(\"SMALLDATETIME\", \"TEXT\")\n",
    "\n",
    "    # execute that SQL with duckdb, this should create the table\n",
    "    cursor.execute(create_table_sql)\n",
    "\n",
    "    # copy CSV into duckdb. CSV is the matching .TXT\n",
    "    path_to_csvpipefile = Path(filename).with_suffix(\".TXT\")\n",
    "    # duckdb copy documentation: https://duckdb.org/docs/sql/statements/copy.html\n",
    "    query = f\"COPY {table_name} FROM '{path_to_csvpipefile}' ( DELIMITER '|')\"\n",
    "    \n",
    "    cursor.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0f26d8-c468-4d7e-ba39-eab88546f205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781e69f0-d9e3-452c-b23b-631a1d9650c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup from https://duckdb.org/docs/guides/python/jupyter.html\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "# No need to import duckdb_engine\n",
    "#  jupysql will auto-detect the driver needed based on the connection string!\n",
    "\n",
    "# Import jupysql Jupyter extension to create SQL cells\n",
    "%load_ext sql\n",
    "%config SqlMagic.autopandas = True\n",
    "%config SqlMagic.feedback = False\n",
    "%config SqlMagic.displaycon = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e1e8a9-eabf-4bed-8c5b-18f87caf6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql duckdb:///duckdb-file.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85aef57-f32e-47cb-b51c-d8908f401fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SHOW TABLES -- no schema name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62426be4-e5fa-4865-b66b-9fea33561b4c",
   "metadata": {},
   "source": [
    "Hey, duckdb implements all the same information schema names as postgres, so one can use the same queries to find the tables with their schaema names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cdc7ae-f14b-4950-a284-ff5753b7a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT schemaname AS schema_name, tablename AS table_name\n",
    "FROM pg_catalog.pg_tables\n",
    "WHERE schemaname != 'pg_catalog'\n",
    "AND schemaname != 'information_schema'\n",
    "ORDER BY schemaname, tablename ASC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50d7a34-35c8-4094-ac43-af150ca7f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM folder_A.TCBC_SUM_1990_JURIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c97eb44-0832-4dc2-88c3-94a02f389b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
