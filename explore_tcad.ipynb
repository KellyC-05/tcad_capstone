{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsK1xq8p8tJXnz1EPTIXvC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TCAD file exploration\n",
        "\n",
        "We have received files from a client.  They are ....\n",
        "\n",
        "# Shorten files for browsing\n",
        "\n",
        "To shorten the files for browsing we can run a short shell script. This opens the zip that was received, and truncates each file at 100 lines long.\n",
        "\n",
        "```{bash, eval=F}\n",
        "# rm -rf shortened_appraisal_files\n",
        "unzip original_data/Appraisal_Roll_History_1990.zip -d shortened_appraisal_files\n",
        "find shortened_appraisal_files -name \"*.TXT\" -exec sed -i.full 100q {} \\;\n",
        "find shortened_appraisal_files -name \"*.TXT.full\" -exec rm {} \\;\n",
        "zip -r shortened_appraisal_files.zip shortened_appraisal_files\n",
        "```\n",
        "\n",
        "We can now attempt to load a shortened file using pandas"
      ],
      "metadata": {
        "id": "UkZzKXgRX3yA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "OaixnN58XrvA",
        "outputId": "1b69c0da-42de-43b5-f72a-4487eb843d4b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b1ba3f2c1d38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shortened_appraisal_files/Appraisal_Roll_History_1990_A/TCBC_SUM_1990_JURIS.TXT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'shortened_appraisal_files/Appraisal_Roll_History_1990_A/TCBC_SUM_1990_JURIS.TXT'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"shortened_appraisal_files/Appraisal_Roll_History_1990_A/TCBC_SUM_1990_JURIS.TXT\", sep = \"|\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Challenge now is to use the *.TDF files to create tables.  I can think of two approaches.\n",
        "\n",
        "1. The TDF files are SQL, so if those are fed to duckdb they should be able to create tables into which the TXT pipe-separated CSV files can be read.  There may be issues with the datatypes not matching (which would require mapping the current datatype definitions to duckdb datatypes by changing the words used to give the datatype to the columns).\n",
        "\n",
        "2. Take the column names out of the TDF files and add them as the column names while reading the relevant CSV files into duckdb.  This would use duckdb's auto understanding of the column datatypes (so it would run, but it might guess wrongly and truncate or change data).\n",
        "\n",
        "I think we should explore step 1 first.\n",
        "\n",
        "## Creating tables using the TDF files\n",
        "\n",
        "We have TDF files scattered through the \\_A and \\_B folders.  I have created a schema (a namespace) for the files from \\_A called \"folder_A\" and \"folder_B\". So there are tables named the same thing in each of the schemas.  You can reference the tables as folder_A.TCBC_SUM_1990_JURIS and folder_B.TCBC_SUM_1990_JURIS \n",
        "\n",
        "We can use python to read each TDF file separately, create the table and then try to load the matching TXT file.  A little guidance on how to process a directory structure of files using Path and glob here:\n",
        "http://howisonlab.github.io/datawrangling/faq.html#get-data-from-filenames"
      ],
      "metadata": {
        "id": "UGoM5byZX_nX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from pathlib import Path\n",
        "import duckdb\n",
        "\n",
        "con = duckdb.connect('duckdb-file.db') #  string to persist to disk\n",
        "cursor = con.cursor()\n",
        "\n",
        "# file_directory = 'shortened_appraisal_files/'\n",
        "file_directory = 'data/'\n",
        "# limit_to_file = 'TCBC_SUM_1990_JURIS'\n",
        "limit_to_file = '*' # all files\n",
        "\n",
        "# create schemas\n",
        "cursor.execute(\"CREATE SCHEMA IF NOT EXISTS folder_A_TCBC;\")\n",
        "cursor.execute(\"CREATE SCHEMA IF NOT EXISTS folder_A_TXBC;\")\n",
        "cursor.execute(\"CREATE SCHEMA IF NOT EXISTS folder_B_TCBC;\")\n",
        "cursor.execute(\"CREATE SCHEMA IF NOT EXISTS folder_B_TXBC;\")\n",
        "# delete schemas that created previously\n",
        "# cursor.execute(\"DROP SCHEMA IF EXISTS folder_A CASCADE\")\n",
        "# cursor.execute(\"DROP SCHEMA IF EXISTS folder_B CASCADE\")\n",
        "\n",
        "for filename in Path(file_directory).rglob(limit_to_file + '.TDF'):\n",
        "    print(filename.parts)\n",
        "    if \"_A\" in filename.parts[1] and \"TCBC_\" in filename.parts[2]:\n",
        "        schema = \"folder_A_TCBC\"\n",
        "    elif \"_A\" in filename.parts[1] and \"TXBC_\" in filename.parts[2]:\n",
        "        schema = \"folder_A_TXBC\"\n",
        "    elif \"_B\" in filename.parts[1] and \"TCBC_\" in filename.parts[2]:\n",
        "        schema = \"folder_B_TCBC\"\n",
        "    elif \"_B\" in filename.parts[1] and \"TXBC_\" in filename.parts[2]:\n",
        "        schema = \"folder_B_TXBC\"\n",
        "    \n",
        "    table_name = schema + \".\" + Path(filename).stem # e.g., A_TCBC_SUM_1990_JURIS\n",
        "\n",
        "    # read .TDF file into string\n",
        "    create_table_sql = Path(filename).read_text()\n",
        "    # Need to alter table name to read in both _A and _B files\n",
        "    create_table_sql = create_table_sql.replace(Path(filename).stem, table_name)\n",
        "    \n",
        "    # Here we have the table creation code in a string, so we can\n",
        "    # swap datatypes out.\n",
        "    # tried SMALLDATETIME --> DATETIME but was still giving errors\n",
        "    # will need to fix this later.\n",
        "    create_table_sql = create_table_sql.replace(\"SMALLDATETIME\", \"TEXT\")\n",
        "    \n",
        "\n",
        "    # execute that SQL with duckdb, this should create the table\n",
        "#     already created table so no need to run\n",
        "#     cursor.execute(create_table_sql) \n",
        "\n",
        "    # copy CSV into duckdb. CSV is the matching .TXT\n",
        "    path_to_csvpipefile = Path(filename).with_suffix(\".TXT\")\n",
        "    # duckdb copy documentation: https://duckdb.org/docs/sql/statements/copy.html\n",
        "    query = f\"COPY {table_name} FROM '{path_to_csvpipefile}' ( DELIMITER '|')\"\n",
        "    cursor.execute(query)"
      ],
      "metadata": {
        "id": "CGkcMOxEX9_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set up sql for dbdocs\n",
        "for filename in Path(file_directory).rglob(limit_to_file + '.TDF'):\n",
        "\n",
        "    # SQL table code with commas\n",
        "    dbdocs_create_table = create_table_sql\n",
        "\n",
        "    # Remove commas before closing parentheses using regular expressions\n",
        "    dbdocs_create_table = dbdocs_create_table.replace(\"),\", \")\")\n",
        "\n",
        "    # Print the updated SQL table code\n",
        "    print(dbdocs_create_table)\n"
      ],
      "metadata": {
        "id": "ZM4OPEyJYGt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup from https://duckdb.org/docs/guides/python/jupyter.html\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "# No need to import duckdb_engine\n",
        "#  jupysql will auto-detect the driver needed based on the connection string!\n",
        "\n",
        "# Import jupysql Jupyter extension to create SQL cells\n",
        "%load_ext sql\n",
        "%config SqlMagic.autopandas = True\n",
        "%config SqlMagic.feedback = False\n",
        "%config SqlMagic.displaycon = False"
      ],
      "metadata": {
        "id": "stBcf3j2YLn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql duckdb:///duckdb-file.db"
      ],
      "metadata": {
        "id": "ba4ZPL06YNWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "SHOW TABLES -- no schema name"
      ],
      "metadata": {
        "id": "_gypNBKuYPlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hey, duckdb implements all the same information schema names as postgres, so one can use the same queries to find the tables with their schaema names."
      ],
      "metadata": {
        "id": "UGI5CXL3YE9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "SELECT schemaname AS schema_name, tablename AS table_name\n",
        "FROM pg_catalog.pg_tables\n",
        "WHERE schemaname != 'pg_catalog'\n",
        "AND schemaname != 'information_schema'\n",
        "ORDER BY schemaname, tablename ASC;"
      ],
      "metadata": {
        "id": "GT1dWHiIYduW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose total of 134933 rows, rows are adding up everytime rerun"
      ],
      "metadata": {
        "id": "xLpnEkVcZ4V3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "SELECT * FROM folder_A_TCBC.TCBC_SUM_1990_JURIS;"
      ],
      "metadata": {
        "id": "mPFp7_qxZ7Js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the data duplicates, then use distinct feature to get correct data"
      ],
      "metadata": {
        "id": "GCRKDFCsZ_uU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "SELECT DISTINCT * FROM folder_A_TCBC.TCBC_SUM_1990_JURIS;"
      ],
      "metadata": {
        "id": "4FcRzOpraBs1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}